{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultinomialNaiveBayes:\n",
    "    \n",
    "    def __init__(self, alpha=1.0):\n",
    "        self.alpha = alpha  \n",
    "        self.classes = None\n",
    "        self.feature_prob = None \n",
    "        self.priors = None\n",
    "    \n",
    "    def fit(self, X, Y):\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        self.classes = np.unique(Y)\n",
    "        n_classes = len(self.classes)\n",
    "        \n",
    "        # Initialize the feature probability matrix\n",
    "        self.feature_prob = np.zeros((n_classes, n_features))\n",
    "        self.priors = np.zeros(n_classes)\n",
    "        \n",
    "        # Calculate prior probabilities and feature probabilities\n",
    "        for idx, c in enumerate(self.classes):\n",
    "            X_c = X[Y == c]  # Samples belonging to class c\n",
    "            self.priors[idx] = X_c.shape[0] / float(n_samples)\n",
    "            \n",
    "            # Add Laplace smoothing (alpha)\n",
    "            feature_count = np.sum(X_c, axis=0) + self.alpha\n",
    "            # Sum of all features for the given class + smoothing for each feature\n",
    "            total_count = np.sum(feature_count)\n",
    "            \n",
    "            # Probability of each feature for the given class\n",
    "            self.feature_prob[idx] = feature_count / total_count\n",
    "    \n",
    "    def predict(self, X):\n",
    "        Y_predicted = [self._predict_sample(x) for x in X]\n",
    "        return np.array(Y_predicted)\n",
    "    \n",
    "    def _predict_sample(self, x):\n",
    "        posteriors = []\n",
    "        \n",
    "        for idx, c in enumerate(self.classes):\n",
    "            # Prior probability (in logarithmic scale)\n",
    "            prior = np.log(self.priors[idx])\n",
    "            \n",
    "            # Calculate posterior probability\n",
    "            # For multinomial distribution, use the sum of x[i] * log(p[i])\n",
    "            feature_prob = self.feature_prob[idx]\n",
    "            # Use only non-zero values of x to speed up calculations\n",
    "            log_likelihood = np.sum(x * np.log(feature_prob))\n",
    "            posterior = prior + log_likelihood\n",
    "            posteriors.append(posterior)\n",
    "        \n",
    "        # Return the class with the highest posterior probability\n",
    "        return self.classes[np.argmax(posteriors)]\n",
    "    \n",
    "    def predict_probability(self, X):\n",
    "        probas = []\n",
    "        \n",
    "        for x in X:\n",
    "            posteriors = []\n",
    "            \n",
    "            for idx, c in enumerate(self.classes):\n",
    "                prior = np.log(self.priors[idx])\n",
    "                feature_prob = self.feature_prob[idx]\n",
    "                log_likelihood = np.sum(x * np.log(feature_prob))\n",
    "                posterior = prior + log_likelihood\n",
    "                posteriors.append(posterior)\n",
    "            \n",
    "            # Convert log probabilities to probabilities\n",
    "            # and normalize so that the sum equals 1\n",
    "            posteriors = np.array(posteriors)\n",
    "            posteriors = np.exp(posteriors - np.max(posteriors))\n",
    "            posteriors = posteriors / np.sum(posteriors)\n",
    "            probas.append(posteriors)\n",
    "            \n",
    "        return np.array(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of your Naive Bayes: 0.9355742296918768\n",
      "Accuracy of sklearn Naive Bayes: 0.9355742296918768\n"
     ]
    }
   ],
   "source": [
    "# 1. Load text data (e.g., 2 categories)\n",
    "categories = ['sci.space', 'rec.sport.baseball']\n",
    "data = fetch_20newsgroups(subset='train', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "# 2. Transform texts into features (bag-of-words)\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(data.data)\n",
    "Y = data.target\n",
    "\n",
    "# 3. Split into training and test sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X_train_np = X_train.toarray()\n",
    "X_test_np = X_test.toarray()\n",
    "\n",
    "# 4. Train your custom classifier\n",
    "my_nb = MultinomialNaiveBayes(alpha=1.0)\n",
    "my_nb.fit(X_train_np, Y_train)\n",
    "my_preds = my_nb.predict(X_test_np)\n",
    "\n",
    "# 5. Train sklearn's MultinomialNB\n",
    "sk_nb = MultinomialNB(alpha=1.0)\n",
    "sk_nb.fit(X_train, Y_train)\n",
    "sk_preds = sk_nb.predict(X_test)\n",
    "\n",
    "# 6. Compare accuracy\n",
    "print(\"Accuracy of your Naive Bayes:\", accuracy_score(Y_test, my_preds))\n",
    "print(\"Accuracy of sklearn Naive Bayes:\", accuracy_score(Y_test, sk_preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
